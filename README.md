# qig_agent
# QIG-Agent æ–‡æ¡£

## å®šä¹‰

QIG-Agent æ˜¯ä¸€ä¸ªé¢å‘å·¥ä¸šæ‰¹é‡ç”Ÿå›¾çš„ Prompt ä¼˜åŒ–æ™ºèƒ½ä½“ï¼š
ç”¨ LangGraph ä¸²èµ· æ£€ç´¢å¢å¼º â†’ è§„åˆ™åŒ–ç”Ÿæˆ â†’ å¤šå€™é€‰è¯„ä¼°/ç­›é€‰ â†’ï¼ˆå°‘é‡ï¼‰ç”Ÿå›¾ â†’ å›¾åƒè¯„ä¼° â†’ å¤±è´¥å½’å› ä¿®å¤ â†’ è®°å¿†å¤ç”¨ çš„é—­ç¯ï¼›è®­ç»ƒæ—¶å†ç”¨ GEPAï¼ˆä»£é™…æ¼”åŒ–æœç´¢ç¯ï¼‰+ PPOï¼ˆç­–ç•¥è’¸é¦ç¯ï¼‰ï¼ŒæŠŠâ€œæœç´¢å‡ºæ¥çš„å¥½ Promptâ€é€æ­¥è’¸é¦è¿›ä¸€ä¸ªå° Policy LLMï¼Œä»è€Œé™ä½åœ¨çº¿è¯•é”™æˆæœ¬ã€æå‡å‡†å…¥ç‡ç¨³å®šæ€§ã€‚

å³ï¼šGEPA è´Ÿè´£æ¢ç´¢ï¼ˆexplorationï¼‰ï¼ŒPPO è´Ÿè´£æŠŠæ¢ç´¢æˆæœå›ºåŒ–æˆå¯å¤ç°èƒ½åŠ›ï¼ˆexploitationï¼‰

1. genetic prompt evolutionï¼ˆé—ä¼ å¼ prompt è¿›åŒ–ï¼‰
2. reflection using natural language feedbackï¼ˆåˆ©ç”¨ç¯å¢ƒäº§ç”Ÿçš„è‡ªç„¶è¯­è¨€åé¦ˆåšåæ€å¼çªå˜ï¼‰

**ä¸€ã€ç³»ç»Ÿæ ¸å¿ƒæ¶æ„**
1. è¿è¡Œæ—¶é—­ç¯ï¼ˆLangGraphï¼‰
2. è®­ç»ƒæ—¶åŒç¯ï¼ˆGEPA æ¢ç´¢ + PPO å›ºåŒ–ï¼‰

**äºŒã€æ ¸å¿ƒåŠŸèƒ½æ¨¡å—**
1. PromptBankï¼ˆRAG æ£€ç´¢å¢å¼ºã€å…ˆéªŒè§„é¿ï¼‰ï¼šsentence-transformers/all-MiniLM-L6-v2ã€FAISSã€SQLite+å‘é‡ç´¢å¼•
2. Prompt Generatorï¼šPolicy LLMï¼ˆnucleus samplingï¼‰ã€GEPA Mutation LLMï¼ˆé—ä¼ å¼è¿›åŒ–ï¼šäº¤å‰/å˜å¼‚ï¼‰
3. Prompt Evaluatorï¼šè§„åˆ™æ ¡éªŒã€0-1åˆ†åˆ¶è¯„åˆ†ã€è‡ªåŠ¨ä¿®å¤
4. Image Generatorï¼šStableDiffusionXLPipelineã€è®¾å¤‡é€‚é…ä¼˜åŒ–
5. Image Evaluatorï¼ˆCascadeï¼‰ï¼šç¡¬çº¦æŸè¿‡æ»¤ã€CLIPå¯¹é½/æ¸…æ™°åº¦/å®¡ç¾æŒ‡æ ‡ã€LLM/VLMæ ¡å‡†ã€é—¨æ§ç­–ç•¥
6. è®­ç»ƒæ¨¡å— - GEPAç¯ï¼šç§ç¾¤è¿›åŒ–ã€ç²¾è‹±ä¿ç•™ã€å¤šä¿çœŸè¯„ä¼°
7. è®­ç»ƒæ¨¡å— - PPOç¯ï¼šClipped PPOã€Value Lossã€KL penaltyã€Advantageè®¡ç®—
8. Skill Selectorï¼šUCB Banditã€å†·å¯åŠ¨ç­–ç•¥ã€è¾¹é™…æ”¶ç›Šæ›´æ–°
9. Skill Libraryï¼š5ç±»Promptä¿®å¤æŠ€èƒ½ï¼ˆadd_neg_textç­‰ï¼‰
10. Objective æ€»åˆ†èåˆï¼šè§„åˆ™èåˆã€é€»è¾‘å›å½’Calibrator

**ä¸‰ã€å…³é”®æ”¯æ’‘æœºåˆ¶**
1. å¤šä¿çœŸè¯„ä¼°ï¼ˆMulti-fidelityï¼‰
2. é‡å¤æƒ©ç½šï¼ˆRepSimï¼šJaccardç›¸ä¼¼åº¦ï¼‰
3. è®°å¿†å¤ç”¨ä¸å…ˆéªŒè§„é¿

![ç³»ç»Ÿæ¶æ„å›¾](Fig.A System Architecture (Layered View))
> å±‚çº§è¯´æ˜ï¼šEvaluation Layer â†’ Application Layer â†’ Agent Layer â†’ Memory & Data Layer

# 1. ğŸ¤”ç³»ç»Ÿæ„æˆï¼šè¿è¡Œæ—¶é—­ç¯ + è®­ç»ƒæ—¶åŒç¯

## 1.1 è¿è¡Œæ—¶é—­ç¯ï¼ˆLangGraphï¼‰

ä¸»å…¥å£ç¤ºä¾‹ï¼š`src/main.py`  
Graph åœ¨ï¼š`src/graph.py`

### âœ… è¿è¡Œæ—¶é—­ç¯å±•ç¤ºå›¾ï¼ˆMermaidï¼‰
```mermaid
flowchart LR
  A[User Query] --> B[Retrieve Memory<br/>PromptBank]
  B --> C[Generate Prompt Candidates<br/>Template + Expand + Prior Avoidance]
  C --> D[Prompt Evaluator<br/>rules + tags + rewrite]
  D --> E[Select Top-K prompts<br/>prompt_min_score]
  E --> F[Image Generator<br/>SDXL Diffusers]
  F --> G[Image Evaluator<br/>Hard Constraints + Metrics]
  G --> H[Accept & Store<br/>results.jsonl + memory]
  H --> I{Decide Loop?<br/>max_iters / thresholds}
  I -- yes --> C
  I -- no --> J[Final Best Result]
```

## 1.2 è®­ç»ƒæ—¶â€œåŒç¯â€ï¼ˆGEPA + PPOï¼‰

è®­ç»ƒè„šæœ¬ï¼ˆåŒç¯ï¼‰åœ¨ï¼š`tools/train_prompt_generator_gepa_ppo.py`  
ï¼ˆPPO-only åœ¨ `tools/train_prompt_generator_ppo.py`ï¼Œå¦‚æœä¸æƒ³ç”¨GEPAï¼‰

- GEPA ç¯ï¼šæ¯ä¸ª query å†…å…ˆåšâ€œprompt æœç´¢/è¿›åŒ–â€
- PPO ç¯ï¼šæŠŠ GEPA æ‰¾åˆ°çš„é«˜ reward promptï¼ˆ+ seedsï¼‰ç”¨ PPO æ›´æ–° policy

### âœ… åŒç¯å±•ç¤ºå›¾ï¼ˆMermaidï¼‰
![åŒç¯è®­ç»ƒå›¾](Fig.2 Training:GEPA (exploration)+PPO (distillation) Double-Loop)

# 2. ğŸ‘æ¨¡å—çš„å…³é”®å®ç°

## 2.1 PromptBankï¼ˆRAG æ£€ç´¢å¢å¼ºã€å…ˆéªŒè§„é¿ï¼‰

æ–‡ä»¶ï¼š`src/memory/prompt_bank.py`

### 1ï¼‰è§£å†³çš„é—®é¢˜
å·¥ä¸šæ‰¹é‡ä»»åŠ¡é‡Œï¼Œâ€œåŒç±»ä»»åŠ¡çš„å¥½ promptâ€å¾€å¾€å¯å¤ç”¨ï¼š
æ£€ç´¢å¢å¼º = æé«˜é¦–è½®å‘½ä¸­ç‡ + å‡å°‘éšæœºè¯•é”™ + ç»™æ¨¡æ¿ç”Ÿæˆæä¾› few-shot è½¯å…ˆéªŒã€‚

### 2ï¼‰ç®—æ³•ç­–ç•¥
- å¥å‘é‡ï¼šsentence-transformers/all-MiniLM-L6-v2
- ç›¸ä¼¼æ£€ç´¢ï¼šFAISSï¼ˆå‘é‡è¿‘é‚»ï¼‰
- å­˜å‚¨ï¼šSQLite + å‘é‡ç´¢å¼•

### 3ï¼‰PromptBank æ„æˆï¼ˆå­˜å‚¨å±‚ + å‘é‡ç´¢å¼•å±‚ï¼‰

#### 1. å­˜å‚¨å±‚ï¼šSQLite è¡¨
```python
CREATE TABLE IF NOT EXISTS prompt_bank (
  id TEXT PRIMARY KEY,
  task_name TEXT, # ä¸åŒä¸šåŠ¡/ä»»åŠ¡éš”ç¦»
  query TEXT, # åŸå§‹ç”¨æˆ·éœ€æ±‚ï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰
  prompt TEXT, # æœ€ç»ˆæœ‰æ•ˆ promptï¼ˆå¯ç”¨äº few-shot/å¤ç”¨ï¼‰
  score REAL, # ç»¼åˆ objective å¾—åˆ†
  failure_tags TEXT, # å¤±è´¥å½’å› æ ‡ç­¾ï¼ˆç”¨äºç»Ÿè®¡ã€å…ˆéªŒè§„é¿ï¼‰
  gen_params TEXT, # ç”Ÿæˆå‚æ•°ï¼ˆsteps/sizeç­‰ï¼Œåšå¯è¿½æº¯ï¼‰
  fixed_prompt TEXT # é¢„ç•™ç»™â€œä¿®å¤åçš„ç‰ˆæœ¬â€ï¼ˆ policy fix / patchï¼‰
)
```

#### 2. å‘é‡å±‚ï¼šSentenceTransformer + FAISSï¼ˆä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢ï¼‰
åŒæ–‡ä»¶é‡Œ PromptBank åˆå§‹åŒ– embedding æ¨¡å‹ï¼š
å¹¶ç”¨ FAISS IndexFlatIP åšæ£€ç´¢ï¼ˆ`src/memory/vector_index.py`ï¼‰
embedding æ—¶ç”¨äº† `normalize_embeddings=True`

**å…³é”®ç‚¹**ï¼š
- å‘é‡å·²å½’ä¸€åŒ–
- IndexFlatIP åšå†…ç§¯ï¼Œæ‰€ä»¥æ£€ç´¢ç­‰ä»·äº cosine similarity
- åŒæ—¶ä¿å­˜ä¸€ä¸ª `.ids` æ–‡ä»¶ï¼ŒæŠŠ â€œFAISS å‘é‡åºå· â†’ sqlite idâ€ å¯¹é½èµ·æ¥

### 4ï¼‰PromptBank å®ç°ï¼ˆä¸¤ä¸ªå…¥å£ï¼‰

#### æ•°æ®æµ
```
user_query
   |
   |  (A) retrieve_similar(query)  â€”â€”>  retrieved_prompts (topKå†å²é«˜åˆ†prompt)
   v
PromptGenerator: ç”Ÿæˆ seeds
   |
   |  (B) prepend few-shot:  retrieved top prompts + å½“å‰æ–°ç”Ÿæˆ prompts
   |  (C) prior avoidance:   ç»Ÿè®¡å†å² failure_tags â†’ è‡ªåŠ¨è¡¥å…… negative
   v
å€™é€‰ prompts è¿›å…¥è¯„ä¼°/ç­›é€‰/ç”Ÿå›¾...
   |
   |  (D) æ»¡è¶³ min_score â†’ å†™å› PromptBankï¼ˆå½¢æˆé—­ç¯è®°å¿†ï¼‰
   v
ä¸‹æ¬¡ query æ›´å®¹æ˜“â€œä¸€æ¬¡å‘½ä¸­â€
```

#### 1. å…¥å£ Aï¼šgraph é‡Œæ£€ç´¢è®°å¿†ï¼ˆretrieve nodeï¼‰
```python
# src/graph.py
db_path = str(Path(state["run_dir"]) / "prompt_bank.sqlite")
bank = PromptBank(db_path)
retrieved = bank.retrieve_similar(task_name=task_name, query=query, top_k=topk)
state["retrieved_prompts"] = retrieved
```
æ¯ä¸ª run_dir ä¸‹éƒ½æœ‰ä¸€ä¸ªç‹¬ç«‹çš„ `prompt_bank.sqlite`ï¼ˆé€‚åˆåšä¸åŒç±»åˆ«ä»»åŠ¡ä¸‹çš„è®­ç»ƒæˆ–å¯¹æ¯”å®éªŒï¼‰ã€‚

#### 2. å…¥å£ Bï¼šPromptGenerator é‡Œåš few-shot + å…ˆéªŒè§„é¿ï¼ˆç”¨ retrievedï¼‰
```python
# src/generators/prompt_generator.py
# (B1) few-shot prependï¼š
if retrieved:
    top = [r["prompt"] for r in retrieved][:2]
    prompts = top + prompts

# (B2) å…ˆéªŒè§„é¿ prior avoidanceï¼ˆåŸºäºå†å²å¤±è´¥ tags æ³¨å…¥é¢å¤– negativeï¼‰
stats = bank.stats_for_task(task_name)
failure_counts = stats.get("failure_tags", {})
neg_extra = build_prior_avoidance_negative(spec, failure_counts, top_k_tags=3)
# è¿½åŠ åˆ° Negative prompt: åé¢
```

### 5ï¼‰PromptBank æ•ˆæœï¼ˆâ€œç¨³å®šæ€§ + å‘½ä¸­ç‡â€ï¼‰

#### æ•ˆæœ 1ï¼šæå‡é¦–è½®å‘½ä¸­ç‡ï¼ˆå‡å°‘æŠ½å¡ï¼‰
ç›¸ä¼¼ query ç›´æ¥å¤ç”¨å†å²é«˜åˆ† promptï¼Œå¾ˆå¤šä»»åŠ¡ä¼šä» â€œè¦ä¿®å¥½å‡ è½®â€ å˜æˆ â€œä¸€è½®å°±è¿‡â€ã€‚

#### æ•ˆæœ 2ï¼šå‡å°‘å…¸å‹å¤±è´¥æ¨¡å¼ï¼ˆå…ˆéªŒè§„é¿ï¼‰
å¤±è´¥æ ‡ç­¾é«˜é¢‘æ„å‘³ç€è¿™æ¡äº§çº¿æœ€å¸¸è¸©å‘ï¼ˆhas_text / realistic_face / blurryâ€¦ï¼‰ã€‚
prior avoidance åšçš„äº‹å°±æ˜¯ï¼šç¬¬ä¸€æ¬¡ç”Ÿæˆå°±æŠŠå‘é¿å¼€ï¼Œä¸ç”¨ç­‰ç”Ÿå›¾åå†è¿”å·¥ã€‚

#### æ•ˆæœ 3ï¼šç»™ PPO/GEPA æä¾›æ›´å¥½çš„â€œèµ·ç‚¹â€
å“ªæ€• policy è¿˜æ²¡å­¦å¥½ï¼Œåªè¦ PromptBank é‡Œç§¯ç´¯äº†è¶³å¤Ÿå¤šé«˜åˆ†æ ·æœ¬ï¼Œseeds è´¨é‡ä¼šæ›´é«˜ï¼š
- GEPA æœç´¢æ›´å¿«æ”¶æ•›ï¼ˆåˆä»£ç§ç¾¤æ›´å¥½ï¼‰
- PPO è’¸é¦æ›´ç¨³å®šï¼ˆteacher æ›´å®¹æ˜“å‡ºç°ï¼‰

####  ablation å®éªŒç»“æœ
| å®éªŒé¡¹ | æŒ‡æ ‡å˜åŒ– |
|--------|----------|
| memory off â†’ memory on | accept_rate 64.7%â†’94.5%ï¼ˆ+29.8%ï¼‰ |
| memory off â†’ memory on | hard_fail_rate 23.2%â†’12.8%ï¼ˆ-10.4%ï¼‰ |
| memory off â†’ memory on | TopKæœ€ä½åˆ† â†‘0.35ï¼ˆè´¨é‡ä¸‹é™æ›´ç¨³ï¼‰ |
| memory off â†’ memory on | å¹³å‡è¿­ä»£è½®æ•° â†“1.2ï¼ˆæ›´å°‘å›ç¯ï¼‰ |
| prior avoidance å…³é—­â†’æ‰“å¼€ | has_text/real_face/blurry å æ¯” 25.6%â†’æ˜¾è‘—ä¸‹é™ |

## 2.2 Prompt Generatorï¼šæ··åˆseedsæ± â€œï¼Œä¸¤æ¡é“¾è·¯å¹¶è¡Œâ€

ï¼ˆPolicy LLM+GEPA mutation LLMï¼‰

### æ··åˆseedsæ± æƒ³æ³•
#### 1) â€œæ··åˆ seeds æ± â€å·¥ä½œåŸç†
##### æ•°æ®æµ
![æ··åˆseedsæ± æ•°æ®æµ](PromptBank retrieve â†’Policy seeds GEPA mutate/crossover Evaluator)
> PromptBankï¼šæä¾›å¥½çš„èµ·ç‚¹ + ç´¯ç§¯å¥½æ ·æœ¬<br>
> Policyï¼šæä¾›å¯å­¦ä¹ çš„â€œé»˜è®¤ç”Ÿæˆå™¨â€<br>
> GEPAï¼šåœ¨éš¾ä¾‹ä¸Šåšæœç´¢ï¼Œç”Ÿæˆæ›´å¥½çš„ teacher prompt<br>
> PPOï¼šæŠŠ teacher çš„æ¨¡å¼å†™å› policy å‚æ•°<br>
> Storeï¼šæŠŠæ–° best prompt å†™å…¥ PromptBankï¼Œé•¿æœŸè®°å¿†ä¸æ–­å¢å¼º

##### å®ç°
###### ğŸ‘‡policy LLM ç”Ÿæˆseeds prompt:
1. ç”¨å›ºå®šæŒ‡ä»¤æ¨¡æ¿æŠŠ user_query åŒ…è£…æˆ prompt_inï¼ˆåŒ…å«è§„åˆ™ä¸ Prompt: è§¦å‘è¯ï¼‰
2. tokenizer ç¼–ç æˆ token ids
3. generate() ç”¨ top-p + temperature é‡‡æ ·ç”Ÿæˆåç»­ tokens
4. decode æˆæ–‡æœ¬
5. ä»ç”Ÿæˆæ–‡æœ¬ä¸­æˆªå– "Prompt:" åé¢çš„éƒ¨åˆ†ä½œä¸ºæœ€ç»ˆ prompt è¾“å‡º
6.ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰å°†è¯¥ prompt ä½œä¸ºå€™é€‰ï¼Œè¿›å…¥ evaluator â†’ reward â†’ PPO update

###### ğŸ‘‡retrieve_memory èŠ‚ç‚¹ï¼š
- policy ç”Ÿæˆ seeds â†’ GEPA ç”¨ LLM æ”¹å†™è¿™äº› seeds â†’ å¾—åˆ° best_prompt
- ä¸»è¦ç”± GEPA mutation/crossover äº§ç”Ÿï¼ŒåŠ å…¥å€™é€‰æ± 
- è¾“å‡ºï¼šmutated/crossover promptsï¼Œå…¶ä¸­ä¸€ä¸ªæˆä¸º best_prompt

###### ğŸ‘‡ç”Ÿæˆå€™é€‰é˜¶æ®µï¼š
- policy ç”Ÿæˆ `policy_seeds = policy.sample(user_query, N)`
- åŒæ—¶æŠŠ retrieved_prompts çš„å‰å‡ æ¡æ’è¿›å€™é€‰é›†

### 2) policy llmï¼ˆ user_query å˜æˆ promptï¼‰

æ–‡ä»¶ï¼š`src/rl/prompt_policy.py`  
ç±»ï¼š`PromptPolicy`
```python
class PromptPolicy:
    """
    PPO policy: ç»™ä¸€ä¸ª user_queryï¼Œè¾“å‡ºä¸€ä¸ªâ€œæ­£å‘ promptâ€ï¼ˆä¸å«å¤šä½™è§£é‡Šï¼‰ã€‚
    """
```

#### 1. å…ˆæ„é€ â€œç»™æ¨¡å‹çš„æŒ‡ä»¤â€ï¼ˆbuild_inputï¼‰
build_input() æŠŠqueryåŒ…è£…æˆä¸€ä¸ªå›ºå®šæ ¼å¼çš„è‹±æ–‡ instructionï¼Œæ ¸å¿ƒæ˜¯è®©è¾“å‡ºå¯æ§
policy çš„â€œè¾“å…¥â€å…¶å®ä¸æ˜¯ user_query æœ¬èº«ï¼Œè€Œæ˜¯ä¸€æ®µç¨³å®šçš„ system-like instruction + User request: ... + Prompt:ã€‚

#### 2. PromptBank ä½œä¸º policy çš„ä¸Šä¸‹æ–‡ few-shot
- PromptBank æ£€ç´¢å‡º 1~3 æ¡ç›¸ä¼¼ query çš„é«˜åˆ† prompt
- æŠŠå®ƒä»¬æ”¾è¿› build_input() é‡Œä½œä¸ºç¤ºä¾‹
- policy ç”Ÿæˆçš„æ–° prompt ä¼šæ›´æ¥è¿‘äº§çº¿çš„é£æ ¼ï¼ˆRAG ç»™â€œç¤ºä¾‹â€ï¼Œpolicy å­¦ç€ä»¿å†™/æ”¹å†™ï¼‰ã€‚

#### 3. PromptBank æä¾› seeds / top prompts
- PromptBank æ£€ç´¢å‡º topK å†å²é«˜åˆ† prompt
- è¿™äº› prompt ç›´æ¥è¿›å…¥å€™é€‰æ± ï¼ˆå’Œ policy seeds æ··åœ¨ä¸€èµ·ï¼‰
- åé¢ç»Ÿä¸€èµ° evaluator / topk ç”Ÿå›¾ / reward / PPO

ğŸ‘‰ è¿™æ¡è·¯çš„æœ¬è´¨ï¼šRAG ç›´æ¥ç»™â€œç­”æ¡ˆå€™é€‰â€ï¼Œpolicy è´Ÿè´£è¡¥å……æ¢ç´¢ã€‚

#### PromptBank å–‚ç»™ policy å½“ few-shot çš„æ ¼å¼
```yaml
You are a professional text-to-image prompt engineer.
Rules: ...ï¼ˆäº§çº¿çš„å·¥ç¨‹è§„åˆ™ï¼‰

Here are high-quality examples for similar requests:
Example 1:
User request: <retrieved_query_1>
Prompt: <retrieved_prompt_1>

Example 2:
User request: <retrieved_query_2>
Prompt: <retrieved_prompt_2>

Now write ONE prompt for:
User request: <user_query>
Prompt:
```

#### 4. tokenizationï¼ˆæŠŠæŒ‡ä»¤ç¼–ç æˆæ¨¡å‹è¾“å…¥ï¼‰

#### 5. ç”¨ generate() é‡‡æ ·ç”Ÿæˆ
```python
outs = self.model.generate(
    **enc,
    max_new_tokens=int(self.cfg.max_new_tokens),
    do_sample=True,
    temperature=float(self.cfg.temperature),
    top_p=float(self.cfg.top_p),
    pad_token_id=self.tokenizer.eos_token_id,
)
```

å¯¹åº” PolicyConfig é»˜è®¤å€¼ï¼š
```python
max_new_tokens = 160
temperature = 0.9
top_p = 0.92
```

**å«ä¹‰**ï¼š
- do_sample=Trueï¼šä¸æ˜¯è´ªå¿ƒ/beamï¼Œè€Œæ˜¯éšæœºé‡‡æ ·ï¼Œä¿è¯æ¢ç´¢
- temperature=0.9ï¼šåå¼€æ”¾ï¼Œé¼“åŠ±å¤šæ ·æ€§
- top_p=0.92ï¼šnucleus samplingï¼Œæˆªæ–­å°¾éƒ¨ä½æ¦‚ç‡è¯ï¼Œé¿å…ç¦»è°±è¾“å‡º
- max_new_tokens=160ï¼šè¾“å‡ºä¸Šé™ï¼Œé˜²æ­¢æ¨¡å‹é•¿ç¯‡è§£é‡Š

#### 6. åå¤„ç†ï¼šåªå– Prompt: åé¢çš„éƒ¨åˆ†
```python
text = self.tokenizer.decode(outs[i], skip_special_tokens=True)
if "Prompt:" in text:
    text = text.split("Prompt:", 1)[-1].strip()
results.append(text.strip())
```

### 3ï¼‰GEPA Mutation LLM
- seed promptï¼šæ¥è‡ª Policy LLMï¼ˆPPO+GEPAè®­ç»ƒï¼‰ï¼ˆæ¯”å¦‚demoçš„ Qwen2.5-1.5B-Instruct + LoRA + PPOï¼‰
- GEPA çš„æ”¹å†™/å˜å¼‚ï¼šæ¥è‡ª å¦ä¸€ä¸ª LLMï¼ˆ qwen 4bï¼‰ï¼ˆé»‘ç›’ï¼Œç”¨æ¥åš mutation/crossoverï¼‰

> policy ç”Ÿæˆ seeds â†’ GEPA ç”¨ qwen4b æ”¹å†™è¿™äº› seeds â†’ å¾—åˆ° best_prompt

#### 1. ä»£é™…æ¼”åŒ–ï¼ˆ population>1ï¼Œå¤šç§ç¾¤å¹¶è¡Œï¼Œé˜²æ­¢å±€éƒ¨æœ€ä¼˜ï¼‰
å½“ population_size>1 æ—¶ï¼Œå…¸å‹æµç¨‹ï¼š
1. åˆå§‹åŒ–ï¼šæ‹¿ policy seeds +ï¼ˆå¯é€‰ï¼‰prompt bank +ï¼ˆå¯é€‰ï¼‰éšæœºå˜å¼‚æ‰©å……
2. è¯„ä¼°æ•´ä»£ï¼šå¯¹æ¯ä¸ª prompt ç®— fitness
3. é€‰ç²¾è‹±ï¼štop-elite_frac ä¿ç•™
4. ç”Ÿæˆä¸‹ä¸€ä»£ï¼š
   - ä»ç²¾è‹±/å‰åŠæ± æŠ½çˆ¶ä»£
   - mutationï¼ˆLLM æ”¹å†™ / skills patchï¼‰
   - crossoverï¼ˆæŠŠä¸¤ä¸ª prompt çš„ä¼˜ç‚¹åˆå¹¶ï¼‰
5. è¿›å…¥ä¸‹ä¸€ä»£ï¼Œé‡å¤ N ä»£

#### 2. promptæ±  åœ¨GEPAåçš„æ„æˆ
è¿è¡Œ/è®­ç»ƒä¸­ä¼šå‡ºç°ä¸¤ç±» promptï¼š

##### A) seedsï¼ˆåˆå§‹å€™é€‰ï¼‰
- promptæ¥è‡ª policy LLM
- ä»£ç è·¯å¾„ï¼š`policy.sample(user_query, ...)`
- è¾“å‡ºï¼šseed prompts

##### B) evolved promptsï¼ˆè¿›åŒ–å‡ºæ¥çš„å€™é€‰ã€å« best_promptï¼‰
- ä¸»è¦ç”± GEPA mutation/crossover äº§ç”Ÿï¼ŒåŠ å…¥å€™é€‰æ± 
- mutation çš„æ¥æºé€šå¸¸æ˜¯qwen 4b
- è¾“å‡ºï¼šmutated/crossover promptsï¼Œå…¶ä¸­ä¸€ä¸ªæˆä¸º best_prompt

**åˆ†å·¥è®¾è®¡**ï¼š
- policyï¼šè´Ÿè´£ç¨³å®šã€ä½æˆæœ¬åœ°äº§ seedï¼ˆå¹¶ä¸”èƒ½è¢« PPO æ›´æ–°ï¼‰
- qwen4bï¼ˆGEPA LLMï¼‰ï¼šè´Ÿè´£æœç´¢/æ”¹å†™

> â€œLLM mutation æ˜¯æœç´¢å™¨ï¼Œpolicy æ˜¯è®°å¿†å™¨/ç”Ÿæˆå™¨â€

## 2.3 Prompt Evaluatorï¼ˆä½æˆæœ¬ reward / tags / rewriteï¼‰

æ–‡ä»¶ï¼š`src/evaluators/prompt_evaluator.py`

### è¾“å…¥/è¾“å‡ºç¤ºä¾‹
- è¾“å…¥ï¼šspec, user_query, prompts
- è¾“å‡ºï¼šæ¯æ¡ prompt ä¸€ä¸ª dictï¼š
  - score âˆˆ [0,1]
  - hard_fail: bool
  - tags: [...]
  - rewrite: str | Noneï¼ˆè‡ªåŠ¨è¡¥é½ negativeï¼‰

### å…³é”®ç­–ç•¥ç¤ºä¾‹
1. å¿…é¡»æœ‰ xxx
2. ç¦æ­¢è§£é‡Š/hashtag
3. é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦æ•° + è¯æ•°ï¼‰
4. ç¦è¯ï¼ˆæ”¿æ²»/æ°´å°/äºŒç»´ç ç­‰ï¼‰
5. å¦‚æœç¼º negativeï¼Œä¼šåš rewrite è¡¥é½

### æ ¸å¿ƒå®ç°ç‰‡æ®µ
```python
# src/evaluators/prompt_evaluator.py
if "Negative prompt:" not in p:
    tags.append("missing_negative")
    hard_fail = True
    rewrite = _ensure_negative(spec, p)

# åŸºç¡€åˆ†ä» 0.60 èµ·ï¼ˆåˆæ ¼çº¿ï¼‰
score = 0.60
if not hard_fail: score += 0.10
if n_words <= 50: score += 0.10
if len(p) <= 240: score += 0.10
if "Negative prompt:" in p and len(neg.strip()) >= 8: score += 0.10
if any(b in p.lower() for b in ban_list): hard_fail=True; score -= 0.40
score = max(0.0, min(1.0, score))
```

## 2.4 Image Generator

æ–‡ä»¶ï¼š`src/generators/image_generator.py`

### å…³é”®ä¿®å¤ï¼ˆæ˜¾å­˜é—®é¢˜ï¼‰
offload æ—¶ decode device ä¸ä¸€è‡´å¯¼è‡´çš„éšæ€§å´©æºƒ/æ˜¾å­˜é—®é¢˜ï¼š
latents decode è¦è·Ÿéš pipe çš„çœŸå®æ‰§è¡Œè®¾å¤‡ï¼ˆpipe._execution_deviceï¼‰
```python
# src/generators/image_generator.py (å…³é”®ç‰‡æ®µ)
self.pipe = StableDiffusionXLPipeline.from_pretrained(model_id, torch_dtype=fp16...)
if self.enable_cpu_offload:
    self.pipe.enable_model_cpu_offload()
...
exec_device = getattr(self.pipe, "_execution_device", self.device)
latents = latents.to(exec_device)  # âœ…å…³é”®ï¼šé¿å… offload æ—¶ device mismatch
```

## 2.5 Image Evaluatorï¼ˆCascadeï¼šç¡¬çº¦æŸ + æŒ‡æ ‡åŒ–åˆ†æ•°ï¼‰

æ–‡ä»¶ï¼š`src/evaluators/image_evaluator.py`

![å›¾åƒè¯„ä¼°å™¨æµç¨‹å›¾](Fig.8 Image Evaluator: Cascade (L1 Hardâ†’L2 Metricsâ†’(optional) L3 Judge))

### Layer1ï¼šHard constraintsï¼ˆç¡®å®šæ€§è¿‡æ»¤ï¼‰
- è¾“å…¥ï¼š prompt + image
- è¾“å‡ºï¼š hard_fail: bool + tags_hard: [...]

**ç¤ºä¾‹çº¦æŸ**ï¼š
- has_text / watermark / logo
- realistic_face / real human face
- severe blur / broken image

**åŸåˆ™**ï¼š Layer1 å¿…é¡»å¿«ã€ç¡®å®šï¼Œç­›å‡ºä¸ç¬¦åˆç¡¬è§„å®šçš„æ•°æ®ã€‚

**å®ç°ç‰‡æ®µ**ï¼š
```python
# src/evaluators/image_evaluator.py (å…³é”®ç‰‡æ®µ)
pen_has_text = 0.60
pen_has_face = 0.90
pen_blurry   = 0.50

if detect_text_simple(img):
   tags.append("has_text"); penalty += pen_has_text; hard_fail=True
if detect_realistic_face(img, thresh=0.65):
   tags.append("has_face"); penalty += pen_has_face; hard_fail=True
if sharpness_raw < 60:
   tags.append("blurry"); penalty += pen_blurry
```

> æ³¨æ„ï¼šè¿™é‡Œ hard_fail=True åªå¯¹ text/faceï¼Œblurry åªæ˜¯ penaltyï¼ˆæ›´æŸ”æ€§ï¼‰ã€‚

### Layer2ï¼šNumerical metricsï¼ˆè¿ç»­ reward ä¿¡å·ï¼‰
- è¾“å…¥ï¼š prompt + imageï¼ˆé€šè¿‡ Layer1 çš„æ ·æœ¬ï¼‰
- è¾“å‡ºï¼š metrics + score_l2 âˆˆ [0,1] + tags_soft

**æŒ‡æ ‡**ï¼š
- clip alignmentï¼ˆè¯­ä¹‰å¯¹é½ï¼‰
- sharpnessï¼ˆæ¸…æ™°åº¦ï¼‰
- aestheticï¼ˆå®¡ç¾/é£æ ¼ proxyï¼‰
- penaltiesï¼ˆå¯¹ tags çš„æƒ©ç½šé¡¹ï¼‰

**åŸåˆ™**ï¼š Layer2 ç»™ PPO/GEPA æä¾›â€œå¯†é›†è¿ç»­â€rewardï¼Œè®­ç»ƒæ‰ä¼šæ”¶æ•›ã€‚

### Layer3ï¼šLLM/VLM judgeï¼ˆç¨€ç–ã€é«˜è´¨é‡æ ¡å‡†ï¼‰
- è¾“å…¥ï¼š user_query + prompt + image + Layer1/2 çš„ tags å’Œ metrics æ‘˜è¦
- è¾“å‡ºï¼š judge_score âˆˆ [0,1] + dims + risk_flags + explanations

**åŸåˆ™**ï¼š Layer3 åªåœ¨å°‘é‡æ ·æœ¬ä¸Šç”¨ï¼Œç”¨æ¥ï¼š
1. æ ¡å‡† Layer2 çš„ç›²åŒºï¼ˆæ¯”å¦‚ aestheticã€CLIP çš„è¯¯åˆ¤ï¼‰
2. åœ¨å€™é€‰ä¹‹é—´â€œè£å†³è°æ›´ç¬¦åˆä¸šåŠ¡å®¡ç¾/ç‰ˆå¼â€
3. äº§ç”Ÿå¯å­¦ä¹ æ•°æ®ï¼ˆpairwise preferenceï¼‰ï¼Œåé¢è®­ç»ƒä¸€ä¸ªä¾¿å®œç‰ˆ ranker é€æ­¥æ›¿ä»£å®ƒ

### é—¨æ§ç­–ç•¥

#### Gate Aï¼šå†³å®šâ€œæ˜¯å¦ç”Ÿå›¾â€ï¼ˆè¿›å…¥ image evaluatorï¼‰
1. å…ˆè¿‡æ»¤ prompt hard_failï¼ˆæ ¼å¼ä¸å¯¹ã€ç¦è¯ç­‰ï¼‰
2. å¯¹å‰©ä½™æŒ‰ prompt_score æ’åº
3. å– topk_img ç”Ÿå›¾
4. å…¶å®ƒå€™é€‰ï¼šåªç”¨ prompt rewardï¼ˆmulti-fidelity çœé’±é˜€é—¨ï¼‰

**ä½œç”¨**ï¼šæ§åˆ¶æ•°æ®æ˜¯å¦è¿›å…¥ Layer1/2ã€‚

#### Gate Bï¼šå†³å®šâ€œæ˜¯å¦è¿›å…¥ Layer3â€
Layer3 çš„é—¨æ§: â€œä¸‰æ®µå¼â€ï¼šç¡¬æ¡ä»¶ + ä¸ç¡®å®šåŒºé—´ + é¢„ç®—ã€‚

##### Gate B çš„è¾“å…¥
- hard_failï¼ˆLayer1ï¼‰
- score_l2ï¼ˆLayer2ï¼‰
- tagsï¼ˆLayer1/2ï¼‰
- å½“å‰æ˜¯ GEPA è¿˜æ˜¯ PPOï¼ˆè®­ç»ƒ/æ¨ç†ï¼‰
- å½“å‰å€™é€‰åœ¨æœ¬è½®çš„æ’å rank
- æœ¬è½®é¢„ç®— budgetï¼ˆeg. æœ€å¤šè°ƒç”¨ 2 æ¬¡ judgeï¼‰

##### Gate B è§„åˆ™
1. hard gateï¼šå¦‚æœ hard_fail=True â†’ ä¸è¿› Layer3ï¼ˆç›´æ¥åˆ¤æ­»ï¼‰
2. uncertainty gateï¼š
   - å¦‚æœ score_l2 >= Ï„_high â†’ ä¸è¿› Layer3ï¼ˆå·²ç»å¾ˆé«˜åˆ†ï¼Œæ²¡å¿…è¦ï¼‰
   - å¦‚æœ score_l2 <= Ï„_low â†’ ä¸è¿› Layer3ï¼ˆå·²ç»å¾ˆå·®ï¼Œæ²¡å¿…è¦ï¼‰
   - åªæœ‰å½“ score_l2 âˆˆ [Ï„_low, Ï„_high] æ‰è¿› Layer3ï¼ˆä¸ç¡®å®šåŒºé—´ï¼‰
3. ranking gateï¼ˆåªè£å†³ç²¾è‹±ï¼‰ï¼šåªå¯¹æœ¬è½® TopMï¼ˆæ¯”å¦‚ Top2 / Top4ï¼‰åš Layer3
4. é¢„ç®— gateï¼šæ¯ä¸ª episode æœ€å¤š judge æ¬¡æ•° Bï¼ˆ1ã€2ã€3....ï¼‰+ å…¨å±€é¢‘ç‡ pï¼ˆæ¯”å¦‚ 3%ï¼‰

### Layer3 çš„èåˆæ–¹å¼
â€œæ ¡å‡†çº åå‹â€èåˆï¼šLayer2 ä»æ˜¯ä¸»ä¿¡å·ï¼ŒLayer3 åªè´Ÿè´£çº å
- è‹¥ä¸è§¦å‘ Layer3ï¼šscore = score_l2
- è‹¥è§¦å‘ Layer3ï¼šscore = (1 - Î±)Â·score_l2 + Î±Â·score_l3ï¼ˆÎ± å– 0.1-0.2 æœ€ä½³ï¼‰

**ä¼˜ç‚¹**ï¼šè®­ç»ƒä»ä»¥å¯†é›† L2 ä¸ºä¸»ï¼ŒL3 æä¾›é«˜è´¨é‡å¯¹é½ï¼Œä¸ä¼šæŠŠ PPO è®­ç»ƒå˜ç¨€ç–ã€‚

### å®Œæ•´æ•°æ®æµ
ä»¥ä¸€æ¬¡ query çš„è®­ç»ƒ episode ä¸ºä¾‹ï¼š
1. policy ç”Ÿæˆ N ä¸ª seeds
2. prompt_evalï¼ˆcheapï¼‰å…¨é‡ â†’ å¾—åˆ° prompt_score / tags
3. Gate Aï¼šé€‰ topk_img ç”Ÿå›¾
4. å¯¹è¿™äº›å›¾è¿›å…¥ image evaluatorï¼š
   - Layer1ï¼šhard detectors â†’ hard_fail / tags_hard
   - Layer2ï¼šcontinuous metrics â†’ score_l2 / tags_soft
   - Gate Bï¼šåˆ¤æ–­æ˜¯å¦è§¦å‘ Layer3
   - Layer3ï¼šjudge â†’ score_l3 / dims / flags
   - èåˆï¼šfinal_score
5. combine_rewardï¼ˆåŠ  length/dup/hard_fail penaltyï¼‰
6. GEPA ç”¨ reward é€‰ç²¾è‹±ï¼›PPO ç”¨ reward æ›´æ–° policy
7. best_prompt / accepted æ ·æœ¬å†™å› prompt bankï¼ˆå½¢æˆé—­ç¯ï¼‰

## 2.6 image score: Objective æ€»åˆ†èåˆ + é€»è¾‘å›å½’èåˆ Calibrator

æ–‡ä»¶ï¼š`src/evaluators/objective.py`

### è§„åˆ™èåˆï¼ˆrule-basedï¼‰
ä» spec é‡Œè¯»æƒé‡ï¼Œç¤ºä¾‹ï¼š
- clip / sharp / aesthetic åŠ æƒåŠ å’Œ
- has_text / realistic_face / blurry ç­‰è§¦å‘å°±æ‰£åˆ†ï¼ˆç”šè‡³ hard_fail ç›´æ¥å‹æ­»ï¼‰

**ä¼˜åŠ¿**ï¼š
- å¯æ§ã€å¯è§£é‡Šã€ä¸Šçº¿ç¨³å®š
- ä½ èƒ½ä¿è¯â€œäº§çº¿çº¢çº¿â€ä¸€å®šè¢«æƒ©ç½š

### å­¦ä¹ å¼èåˆï¼ˆreward calibratorï¼Œå¯é€‰ï¼‰
å¦‚æœå­˜åœ¨ `configs/reward_calibrator.joblib`ï¼Œobjective ä¼šé¢å¤–ï¼š
1. æŠŠå¤šç»´æŒ‡æ ‡æ‹¼æˆ feature å‘é‡ x
2. ç”¨é€»è¾‘å›å½’ï¼ˆæˆ–ä½ è®­ç»ƒçš„æ¨¡å‹ï¼‰è¾“å‡ºä¸€ä¸ªæ¦‚ç‡ p = P(accept|x)
3. å†æŠŠå®ƒå’Œ rule score blendï¼šS = (1 - Î±)Â·S_rule + Î±Â·p

**æ„ä¹‰**ï¼š
- è®©â€œèåˆæƒé‡â€ä»æ‰‹å†™å˜æˆè´´è¿‘çœŸå®å‡†å…¥è§„åˆ™
- å¯¹ä¸€äº›å¤æ‚å¤±è´¥æ¨¡å¼æ›´æ•æ„Ÿ

### å…³é”®æ•ˆæœå±•ç¤º
- æˆåŠŸç‡/å‡†å…¥ç‡ï¼šresults.jsonl é‡Œçš„ scoreã€hard_failã€tags
- å¤±è´¥æ¨¡å¼åˆ†å¸ƒï¼štags ç»Ÿè®¡ï¼ˆblurry/has_text/has_faceï¼‰
- é—­ç¯æ˜¯ä¸æ˜¯ç”Ÿæ•ˆï¼šloop_iter å¢é•¿ã€autofix ç”Ÿæ•ˆï¼ˆprompt rewriteï¼‰
- æ˜¯å¦æŠ½å¡é™ä½ï¼šTopK çš„æœ€ä½åˆ†æ˜¯å¦ä¸Šå‡ï¼ˆè´¨é‡ä¸‹é™æ›´ç¨³ï¼‰

# 3. åŒç¯è®­ç»ƒï¼šPPO + GEPA çš„æ„é€ ã€å…¬å¼ã€å‚æ•°ã€æ•°æ®æµ

## 3.1 è®­ç»ƒæ•°æ®æ„é€ 

#### A) results.jsonlï¼ˆè¿è¡Œæ—¶/è¯„ä¼°äº§ç”Ÿï¼‰
- Graph æ¯æ¬¡ run ä¼š append è®°å½•ï¼ˆåŒ…æ‹¬ metrics/tags/scoreï¼‰
- Reward calibrator è®­ç»ƒè„šæœ¬ä¼šåœ¨ runs/ ä¸‹é€’å½’æœé›†æ‰€æœ‰ results.jsonl

å¯¹åº”è„šæœ¬ï¼š`tools/train_reward_calibrator.py`
```python
for p in runs.glob("**/results.jsonl"):
  for line in p: items.append(json.loads(line))
```

#### B) train_log.jsonlï¼ˆè®­ç»ƒæ—¶ PPO/GEPA äº§ç”Ÿï¼‰
åŒç¯è„šæœ¬ä¼šå†™è®­ç»ƒæ—¥å¿—ï¼ˆç”¨äº `tools/analyze_train_log.py` åˆ†æï¼‰ã€‚

## 3.2 Reward æ€»å…¬å¼
Reward ç»„åˆåœ¨ï¼š`src/rl/reward.py`

è¡¨è¾¾å¼ï¼š
$$ R = w_{prompt} \cdot S_{prompt} + w_{image} \cdot S_{image} \cdot \mathbb{I}_{gen} - Penalty_{hard\_fail} - Penalty_{len} - Penalty_{rep} $$

å…¶ä¸­ï¼š
- $S_{prompt}$ï¼šPrompt Evaluator çš„åˆ†æ•°ï¼ˆ[0,1]ï¼‰
- $S_{image}$ï¼šImage Evaluator çš„ objective åˆ†æ•°ï¼ˆ[0,1]ï¼‰
- $\mathbb{I}_{gen}$ï¼šæ˜¯å¦è¿›å…¥ TopK ç”Ÿå›¾ï¼ˆmulti-fidelity çœé’±é˜€é—¨ï¼‰
- $Penalty_{rep}$ï¼šé‡å¤æƒ©ç½šï¼ˆRepSimï¼šJaccard ç›¸ä¼¼åº¦ï¼‰
- $Penalty_{len}$ï¼šé•¿åº¦åç¦» target çš„æƒ©ç½šï¼ˆæ§åˆ¶ CLIP/SDXL é€‚é…ï¼‰

## 3.3 Multi-fidelity é—¨æ§é˜€é—¨
åŒç¯è®­ç»ƒä¸­ï¼š
- å¯¹å…¨éƒ¨å€™é€‰å…ˆç®— prompt_score
- åªå¯¹å‰ topk_to_generate_images çš„å€™é€‰è·‘ ç”Ÿå›¾ + image_eval

é…ç½®æ–‡ä»¶ï¼ˆ`configs/gepa_ppo_prompt.yaml`ï¼‰ï¼š
```yaml
multi_fidelity:
  num_prompt_candidates: xx
  topk_to_generate_images: xx
  images_per_prompt: xxx
```

## 3.4 RepSimï¼ˆé‡å¤æƒ©ç½šï¼‰
åŒç¯è®­ç»ƒé‡Œç”¨ Jaccard ç›¸ä¼¼åº¦è¿‘ä¼¼é‡å¤åº¦ï¼š
$$ RepSim = \frac{|A \cap B|}{|A \cup B|} $$
å†è¿›å…¥ rewardï¼š$Penalty_{rep} = \lambda_{rep} \cdot RepSim$

**ä¼˜ç‚¹**ï¼š
- ä¸ç”¨ embeddingï¼Œä¸ç”¨é¢å¤–æ¨¡å‹
- çº¯é›†åˆè¿ç®—ï¼Œå¿«é€Ÿã€æˆæœ¬ä½
- å¯¹â€œæ¨¡æ¿åŒ– prompt å˜æˆä¸€æ¨¡ä¸€æ ·â€çš„é€€åŒ–æœ‰å¼ºæŠ‘åˆ¶

## åŒç¯æ•°æ®æµæ€»å›¾
```
user_query
   â”‚
   â–¼
Policy LLM (sample K prompts)  â”€â”€â”€â”€â”€â”€â–º seeds[]
   â”‚                               â”‚
   â”‚                               â–¼
   â”‚                        GEPA Optimize (E generations)
   â”‚                               â”‚
   â”‚                               â–¼
   â”‚                         best_prompt, best_reward
   â”‚                               â”‚
   â–¼                               â–¼
PPO Rollout batch = [best_prompt + seeds]
   â”‚
   â–¼
PromptEval (cheap) -> topK -> ImageGen+ImageEval (expensive)
   â”‚
   â–¼
Reward shaping (combine_reward)
   â”‚
   â–¼
PPO Update -> Policy LLM improves
   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ next update repeats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

# 4. PPO ç¯ï¼šå…¬å¼ã€æ­¥éª¤

PPO æ›´æ–°åœ¨ï¼š`tools/train_prompt_generator_gepa_ppo.py` çš„ `ppo_update_step`
å®ç°ï¼š clipped PPO + value loss + KL penaltyã€‚

## 4.1 PPO çš„å…³é”®å®šä¹‰

#### (1) Advantageï¼ˆdemoï¼šä¸€æ­¥ advantageï¼‰
$$ A = R - V_{old}(s) $$

#### (2) Ratio
$$ r(Î¸) = exp\left( log \pi_Î¸(a|s) - log \pi_{Î¸_{old}}(a|s) \right) $$

#### (3) Clipped surrogate objective
$$ L_{clip} = -\mathbb{E}\left[ min\left( r(Î¸) \cdot A, clip(r(Î¸), 1-\epsilon, 1+\epsilon) \cdot A \right) \right] $$

#### (4) Value loss
$$ L_V = \mathbb{E}\left[ (V(s) - R)^2 \right] $$

#### (5) Approx KL penalty
$$ KL \approx \mathbb{E}\left[ log \pi_{Î¸_{old}}(a|s) - log \pi_Î¸(a|s) \right] $$

#### æœ€ç»ˆ loss
$$ L = L_{clip} + c_V \cdot L_V + c_KL \cdot KL $$

> â€œæŠŠ reward å’Œ baselineï¼ˆvalueï¼‰åšå·®ï¼Œå¾—åˆ°è¿™æ¬¡ç”Ÿæˆçš„ prompt ç›¸å¯¹å¹³å‡æ°´å¹³çš„å¥½åï¼›ç„¶åç”¨ PPO çš„ ratio+clip æœºåˆ¶ï¼Œç¨³å¥åœ°æé«˜å¥½ prompt çš„ç”Ÿæˆæ¦‚ç‡ã€é™ä½å·® prompt çš„ç”Ÿæˆæ¦‚ç‡ï¼ŒåŒæ—¶è®­ç»ƒ value head å˜å¾—æ›´å‡†ï¼Œå¹¶ç”¨ KL/clip é˜²æ­¢æ›´æ–°è¿‡çŒ›ã€‚â€

## 4.2 åç»­æå‡ï¼šæŠŠâ€œä¿®å¤é—­ç¯â€çº³å…¥ policyï¼ˆçœŸæ­£å¤šæ­¥ ï¼Œç”¨GAE ï¼‰

###  episode å®šä¹‰
â€œæœ€å¤š T è½®çš„æç¤ºè¯ä¼˜åŒ–â€ï¼Œpolicy åœ¨ä¸€ä¸ª episode é‡Œå¤šæ¬¡å‡ºæ‰‹ï¼Œå½¢æˆï¼š
- $s_t$ï¼šåŒ…å« query + å½“å‰ prompt + æœ€è¿‘ä¸€æ¬¡ tags/metrics + é¢„ç®—å‰©ä½™ï¼ˆæ¬¡æ•°ï¼‰
- $a_t$ï¼špolicy è¾“å‡ºâ€œä¸‹ä¸€ç‰ˆ promptâ€
- $r_t$ï¼šæœ¬è½® rewardï¼ˆprompt_score + image_score çš„ç»„åˆï¼‰
- $done$ï¼šè¾¾åˆ° accept / è¾¾åˆ° max_steps / hard_fail ç»ˆæ­¢

### çŠ¶æ€è¾“å…¥
æ¯ä¸€æ­¥è¾“å…¥ç»™ policy çš„æ–‡æœ¬åŒ…å«ï¼š
- åŸå§‹ user_query
- å½“å‰ promptï¼ˆä¸Šä¸€è½®è¾“å‡ºï¼‰
- ä¸Šä¸€è½®è¯„ä¼°æ‘˜è¦ï¼štags + å…³é”®æ•°å€¼ï¼ˆclip/sharp/aesthetic/scoreï¼‰
- è¿˜å‰©å‡ è½®é¢„ç®—
- ä»»åŠ¡ç¡¬çº¦æŸï¼ˆno text/no real faceâ€¦ï¼‰

### è½¨è¿¹å¥–åŠ±
- æ¯ä¸€æ­¥ rewardï¼šç”¨ç°æœ‰ combine_reward()ï¼ˆcheap+expensive å¤šä¿çœŸï¼‰
- ç»ˆæ­¢æ—¶ bonusï¼š
  - accept ç»™ +bï¼ˆä¾‹å¦‚ +0.5ï¼‰
  - è¶…é¢„ç®—/å¤±è´¥ç»™ -bï¼ˆä¾‹å¦‚ -0.5ï¼‰

### GAE å…¬å¼
å¯¹æ¯æ¡ episodeï¼Œå€’åºç®—ï¼š
$$ \delta_t = r_t + (1 - d_t) \gamma V(s_{t+1}) - V(s_t) $$
$$ A_t = \delta_t + \gamma (1 - d_t) \lambda A_{t+1} $$
$$ R_{return} = A_t + V(s_t) $$

å…¶ä¸­ $d_t=1$ è¡¨ç¤ºç»ˆæ­¢ã€‚

### PPO æ›´æ–°
- advantages åš normalize
- returns ç”¨æ¥è®­ç»ƒ value headï¼ˆMSEï¼‰
- PPO clipped loss å½¢å¼ä¸å˜ï¼š
  $$ L_{clip} = -\mathbb{E}\left[ min\left( r(Î¸) \cdot A_t, clip(r(Î¸), 1-\epsilon, 1+\epsilon) \cdot A_t \right) \right] $$
- value lossï¼š
  $$ L_V = \mathbb{E}\left[ (V(s_t) - R_{return})^2 \right] $$

# 5. GEPA ç¯ï¼šç”Ÿæˆå¼è¿›åŒ–ç®—æ³•ç¯

GEPA åœ¨ï¼š`src/llm/gepa_optimizer.py`

- Populationï¼šä¸€ç»„ prompt
- Fitnessï¼šreward
- Selectionï¼šä¿ç•™ topN
- Crossoverï¼šæ‹¼æ¥/äº¤æ¢ prompt ç‰‡æ®µï¼ˆè®©å­ä»£ç»§æ‰¿ä¼˜ç‚¹ï¼‰
- Mutationï¼ˆä¸¤ç±»ï¼‰ï¼š
  a. skill mutationï¼šçœ‹åˆ° tag å°±æ‰§è¡ŒæŠ€èƒ½ä¿®å¤
  b. llm mutationï¼šè°ƒç”¨ LLM ç›´æ¥ rewriteï¼ˆæ›´å¼ºä½†æ›´è´µï¼‰

å¹¶ä¸”å®ƒä¼šæŠŠ patch è®°å½•è¿› memoryï¼ˆ`TrajectoryMemory.record_patch()`ï¼‰ï¼Œç”¨äºå¤ç›˜åˆ†æã€‚

## 5.1 GEPA çš„æ ¸å¿ƒå¯¹è±¡ï¼ˆdataclassï¼‰
```python
@dataclass
class GEPAConfig:
    population_size: int = 12
    generations: int = 5
    elite_size: int = 3
    crossover_rate: float = 0.35
    mutation_rate: float = 0.90
    llm_mutation_rate: float = 0.65
    max_words: int = 45
    image_eval_topk: int = 3
    images_per_prompt: int = 1
```

> å¯ä»¥çœ‹åˆ°ï¼šGEPAConfig æœ¬èº«å·²ç»æ˜¯ä¸€ä¸ªæ¸…æ™°çš„â€œè¿›åŒ–æœç´¢æ§åˆ¶é¢æ¿â€ã€‚

## 5.2 GEPA çš„æ¯ä¸€ä»£ï¼ˆgenerationï¼‰æ¼”åŒ–è¿‡ç¨‹

### 1ï¼‰ç§ç¾¤ä¸ç²¾è‹±ä¿ç•™
- åˆå§‹åŒ–ç§ç¾¤ = seeds + ä¸€äº›å˜å¼‚æ‰©å……
- æ¯ä¸€ä»£ï¼š
  1. è¯„ä¼°æ•´ä¸ª promptæ± ï¼ˆprompt_eval å…¨é‡ + topk ç”Ÿå›¾ï¼‰
  2. å¯¹äºæ¯ä¸ªç§ç¾¤ï¼Œå–å‰ elite_frac ä½œä¸ºç²¾è‹±
  3. ç”¨ç²¾è‹±ç”Ÿæˆä¸‹ä¸€ä»£ï¼ˆcrossover + mutationï¼‰ï¼šä½¿ç”¨LLMåšå˜å¼‚/æ”¹å†™ç­‰ã€‚

### 2ï¼‰crossover / mutation çš„æ¥æºï¼šLLM + fallback skills
GEPA çš„å…³é”®åˆ›æ–°ç‚¹æ˜¯ï¼šä¸æ˜¯éšæœºå™ªå£°çªå˜ï¼Œè€Œæ˜¯â€œLLM ç”Ÿæˆå¼çªå˜â€ï¼š
- ä¼˜å…ˆç”¨ LLM åšâ€œè¯­ä¹‰åˆç†çš„å˜å¼‚â€
- å¦‚æœ LLM è¾“å‡ºç©º/æ ¼å¼å¼‚å¸¸ï¼Œå°± fallback åˆ° rule-based æŠ€èƒ½åº“ patch

### 3ï¼‰GEPA çš„ multi-fidelity è¯„ä¼°
`_eval_population()` çš„é€»è¾‘æ˜¯ï¼š
- å¯¹å…¨ä½“ prompts åš prompt_score
- åªå¯¹å‰ topk_img åšç”Ÿå›¾ + image_score
- ç„¶åç”¨ combine_reward() å¾—åˆ°æ¯æ¡ prompt çš„ reward
- å¾—åˆ†è¾¾åˆ°é˜ˆå€¼çš„å­ä»£åŠ å…¥promptæ± 

## ç¤ºä¾‹ï¼šGEPA æœç´¢ â†’ PPO è’¸é¦æµç¨‹

### åœºæ™¯è®¾å®š
user_queryï¼š
> â€œç”Ÿæˆä¸€å¼ 3dé£æ±‰æœå°†å†›çš„å¨ä¸¥ç”»åƒï¼Œå†™å®æ’ç”»é£æ ¼ï¼Œä¸èƒ½æœ‰æ–‡å­—ï¼Œä¸èƒ½åƒçœŸäººè„¸ï¼ŒèƒŒæ™¯ä¸è¦å¤æ‚ï¼Œé«˜æ¸…â€

å·¥ä¸šçº¦æŸï¼š
- ç¦æ­¢æ–‡å­—ï¼ˆhas_textï¼‰
- ç¦æ­¢é€¼çœŸçœŸäººè„¸ï¼ˆrealistic_faceï¼‰
- ä¸è¦æ¨¡ç³Šï¼ˆblurryï¼‰
- prompt è¦ç´§å‡‘ã€ä¸è¦è§£é‡Š
- æœ€å¥½æœ‰è´Ÿå‘è¯ï¼ˆnegative promptï¼‰

### Step 1ï¼šPolicy LLM å…ˆäº§ seedsï¼ˆåˆå§‹å¯èƒ½å¾ˆæ™®é€šï¼‰
å‡è®¾é‡‡æ ·å‡º 4 ä¸ª seedsï¼š
| seed | prompt å†…å®¹ | prompt_eval å¾—åˆ† | tags |
|------|-------------|------------------|------|
| S1 | "A powerful Han dynasty general, realistic illustration, detailed armor, cinematic lighting, ultra high resolution." | 0.65 | missing_negative |
| S2 | "Portrait of a Han dynasty general, stern expression, realistic style, high detail, sharp focus." | 0.60 | missing_negative, risk_realistic_face |
| S3 | "Han dynasty general standing, historical costume, epic atmosphere, realistic painting." | 0.62 | missing_negative |
| S4 | "A realistic Han dynasty warrior general, close-up, highly detailed face, 8k." | 0.58 | missing_negative, high_risk_realistic_face |

> æ­¤æ—¶ policy æ²¡å­¦ä¼šâ€œçº¦æŸè¾“å‡ºâ€ï¼Œç›´æ¥ç”Ÿå›¾å¯èƒ½è§¦å‘ hard_failã€‚

### Step 2ï¼šGEPA å¼€å§‹â€œè¿›åŒ–æœç´¢â€ï¼ˆteacher çš„æ¥æºï¼‰
GEPA æŠŠ seeds å½“åˆä»£ç§ç¾¤ï¼Œé’ˆå¯¹ tags åš mutation / crossoverï¼š

#### 3.1 ä¾æ® tag åš mutation
ä» S2 å˜å¼‚å‡º M1ï¼ˆæ¶ˆæ‰é£é™©ç‚¹ï¼‰ï¼š
> "A stern Han dynasty general in ornate lamellar armor, heroic and stylized, realistic illustration but not photorealistic, cinematic rim light, clean simple background, sharp details, high quality. Negative prompt: text, watermark, logo, photorealistic face, realistic portrait, extra fingers, blurry."

ä» S1 å˜å¼‚å‡º M2ï¼š
> "Han dynasty general, historical armor, heroic posture, realistic illustration, no inscriptions, minimal background, high clarity, cinematic lighting. Negative prompt: any text, watermark, logo, signature, photorealistic face, close-up portrait, blur."

#### 3.2 crossover äº¤å‰ï¼ˆæ‹¼æ¥ä¼˜ç‚¹ï¼‰
S1 çš„â€œå¨ä¸¥+ç”µå½±å…‰â€ + M1 çš„â€œéå†™å®è„¸+è´Ÿå‘è¯â€ â†’ C1ï¼š
> "A mighty Han dynasty general, imposing presence, ornate lamellar armor, epic cinematic lighting, realistic illustration stylized, clean background, high sharpness. Negative prompt: text, watermark, logo, signature, photorealistic face, portrait closeup, blur, lowres."

### Step 4ï¼šGEPA å†è¯„ä¼°ï¼Œé€‰å‡º best_promptï¼ˆteacherï¼‰
| å€™é€‰ | prompt å†…å®¹ | prompt_eval å¾—åˆ† | image_score | æœ€ç»ˆ reward |
|------|-------------|------------------|-------------|-------------|
| C1 | ä¸Šè¿° crossover ç»“æœ | 0.85 | 0.74 | 0.74 |
| M1 | ä¸Šè¿° mutation ç»“æœ | 0.80 | 0.68 | 0.66 |
| M2 | ä¸Šè¿° mutation ç»“æœ | 0.78 | 0.65 | 0.63 |

> best_prompt = C1ï¼ˆé«˜ rewardï¼Œæ—  hard_failï¼‰

### Step 5ï¼šPPO è’¸é¦ teacher
PPO batch åŒ…å« [C1 + S1~S4]ï¼Œæ ¹æ® reward æ›´æ–° policyï¼š
- C1 é«˜ reward â†’ adv æ­£ â†’ PPO æé«˜å…¶ç”Ÿæˆæ¦‚ç‡
- S2/S4 ä½ reward â†’ adv è´Ÿ â†’ PPO é™ä½å…¶ç”Ÿæˆæ¦‚ç‡

### Step 6ï¼šä¸‹ä¸€è½® policy è¾“å‡ºå˜åŒ–
è®­ç»ƒåï¼Œpolicy ç›´æ¥ç”Ÿæˆç¬¦åˆçº¦æŸçš„ promptï¼š
> "A mighty Han dynasty general, imposing presence, ornate lamellar armor, epic cinematic rim lighting, stylized realistic illustration, clean minimal background, high sharpness. Negative prompt: text, watermark, logo, photorealistic face, portrait closeup, blur."

# 6 Skill Selectorï¼šUCBï¼ˆbanditï¼‰é€‰æŠ€èƒ½

src/agent/skill_selector.py

æ ¸å¿ƒæ˜¯ UCBï¼š
$$ UCB(i) = \bar{r}_i + c \cdot \sqrt{\frac{log(N)}{n_i}} $$
- $\bar{r}_i$ï¼šæŠ€èƒ½içš„å†å²å¹³å‡æ”¶ç›Š(æˆåŠŸ/score)
- $n_i$ï¼šæŠ€èƒ½iè¢«é€‰æ¬¡æ•°
- $N$ï¼šæ€»å°è¯•æ¬¡æ•°
- $c$ï¼šæ¢ç´¢ç³»æ•°(é…ç½®é‡Œå«ucb_c)

å¯¹åº”ä»£ç ï¼ˆå…³é”®è¡Œï¼‰ï¼š
```python
# src/agent/skill_selector.py
bonus = self.c * math.sqrt(math.log(max(1, self.total_tries)) / float(st.n))
v = st.mean_gain + bonus
```

## 6.1 cold-start å†·å¯åŠ¨
```python
for s in skill_names:
    if self.stats.get(s, SkillStats()).n == 0:
        return s
```
> åŸå› ï¼šæŸä¸ªæŠ€èƒ½æ²¡ç”¨è¿‡ï¼Œä¼˜å…ˆè¯•ä¸€æ¬¡ï¼Œé¿å…å±€éƒ¨æœ€ä¼˜ã€‚

## 6.2 æ¢ç´¢ç³»æ•°c
```python
# tools/train_prompt_generator_gepa_ppo.py
selector = SkillSelector(c=float(cfg["skills"].get("ucb_c", 1.4)))
```

## 6.3 â€œreward_gainâ€
UCB ç”¨çš„æ˜¯è¾¹é™…æ”¶ç›Šï¼š
$$ reward\_gain = R_{after} - R_{before} $$

#### æµç¨‹ï¼š
1. é€‰ skill æŠ€èƒ½
2. apply skill å¾—åˆ°æ–° prompt
3. é‡æ–°è¯„ä¼°ï¼ˆprompt_score / image_score / rewardï¼‰
4. gain = after - before
5. selector.update(skill, gain) æ›´æ–°å‡å€¼ä¸æ¬¡æ•°

## 6.4 å·¥å…·/æŠ€èƒ½ï¼ˆSkillLibrary ï¼‰

æ–‡ä»¶ï¼š`src/agent/skill_library.py`ï¼Œå†…ç½® 5 ä¸ªæŠ€èƒ½ï¼š

### â‘  add_neg_textï¼šå¼ºåŠ›ç¦æ­¢æ–‡å­—/æ°´å°
- è§¦å‘ tagsï¼šneed_neg_text, hard_forbid_text, pos_mentions_text_overlay
- ä½œç”¨ï¼šå¾€ negative prompt é‡Œè¡¥å…³é”®è¯
```python
name="add_neg_text"
tokens=["text","watermark","logo","signature","caption","subtitle"]
```

### â‘¡ add_neg_realistic_faceï¼šåªç¦çœŸäººè„¸
- è§¦å‘ tagsï¼šneed_neg_realistic_face, hard_forbid_realistic_face, pos_mentions_real_face
```python
tokens=[
 "real human","portrait photo","photorealistic face",
 "realistic face","human face","skin texture","real person"
]
```

### â‘¢ remove_pos_text_wordsï¼šåˆ é™¤æ­£å‘â€œå¼•å¯¼å‡ºå­—â€çš„è¯
- è§¦å‘ tagsï¼špos_mentions_text_overlay
- ä½œç”¨ï¼šæ­£å‘ prompt åˆ é™¤ watermark/logo/text ç­‰

### â‘£ expand_promptï¼šå¤ªçŸ­è¡¥è´¨é‡è¯+æ„å›¾è¯
- è§¦å‘ tagsï¼šprompt_too_short
- ä½œç”¨ï¼šåŠ å›ºå®šå¢å¼ºæè¿°ï¼ˆé«˜è´¨é‡ã€ç”µå½±å…‰ã€å¹²å‡€æ„å›¾ç­‰ï¼‰

### â‘¤ shorten_promptï¼šå¤ªé•¿æˆªæ–­
- è§¦å‘ tagsï¼šclip_trunc_risk
- ä½œç”¨ï¼šæ§åˆ¶æ­£å‘ prompt æœ€å¤§è¯æ•°ï¼ˆé»˜è®¤ 110ï¼‰

## 6.5 æŠ€èƒ½è°ƒç”¨é€»è¾‘
åœ¨ `src/llm/gepa_optimizer.py` ä¸­ï¼ŒGEPA åš mutation æ—¶ï¼š
1. LLM æ‰¾åˆ°ä¸ tags åŒ¹é…çš„å¯ç”¨æŠ€èƒ½ï¼š`available_skills_for_tags(tags)`
2. è§„èŒƒåŒ–æˆæŠ€èƒ½ååˆ—è¡¨
3. åº”ç”¨æŠ€èƒ½

## 6.6 UCBåœ¨mutationä¸­çš„æ¼”è¿›
ä¼ªé€»è¾‘ï¼š
```
parent_reward = reward_map[parent]["reward"]
child_reward = evaluate(child)
gain = child_reward - parent_reward
selector.update(skill, gain)
```
UCB ä¼šé€æ¸åå¥½å¯¹æŸç±» tags æœ€æœ‰æ•ˆçš„ skillã€‚

---

### å›¾ç‰‡å­˜æ”¾è¯´æ˜
å°†æ–‡æ¡£ä¸­çš„å›¾ç‰‡ï¼ˆFig.Aã€Fig.2ã€Fig.8 ç­‰ï¼‰ä¿å­˜åˆ°ä»“åº“çš„ `assets/img/` ç›®å½•ä¸‹ï¼Œç„¶åä¿®æ”¹å›¾ç‰‡å¼•ç”¨è·¯å¾„ä¸ºï¼š
```markdown
![å›¾ç‰‡æè¿°](assets/img/å›¾ç‰‡æ–‡ä»¶å.png)
```
ç¡®ä¿ GitHub èƒ½æ­£å¸¸åŠ è½½å›¾ç‰‡ã€‚
